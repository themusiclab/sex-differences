---
bibliography: citations.bib
csl: nature.csl
header-includes:
- \usepackage[left]{lineno}
- \usepackage{ragged2e}
- \usepackage{caption}
- \usepackage{longtable}
- \usepackage[labelformat = empty]{caption}
- \usepackage{afterpage}
- \usepackage{fontenc}
- \usepackage{soul}
- \usepackage{xcolor}
- \usepackage[symbol]{footmisc}
- \usepackage{numprint}
- \usepackage{lineno}
- \npthousandsep{,}
- \definecolor{bleu}{HTML}{2200cc}
- \renewcommand{\thefootnote}{\fnsymbol{footnote}}
notes-after-punctuation: no
urlcolor: bleu
linkcolor: bleu
output:
  pdf_document: default
editor_options:
  markdown:
    wrap: sentence
---

```{r chunk_knit_settings, include=F}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = FALSE)
options(scipen = 999) # disable scientific notation
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
####################################################################
# There are a number computations that take a long time to run
# For convenience, these computations are parameterized such that 
# they are not run each time the manuscript is knitted by default
# and instead just load the last saved run.
# If you want to run these, set full_run below to TRUE.
# Otherwise, do nothing.
####################################################################
full_run <- FALSE
if (full_run == TRUE) {
  source(here("analysis", "statistical", "lasso.R"))
}
```

```{r load_libraries_and_functions}
# if manuscript not knitting run this code separately, and make sure all packages are installing correctly
library(pacman)
p_load(
  BayesFactor,
  broom,
  broom.mixed,
  cowplot,
  effectsize,
  gamlss,
  ggpubr,
  heplots,
  here,
  janitor,
  kableExtra,
  knitr,
  lavaan,
  partykit,
  png,
  psych,
  semPlot,
  tidyverse
)

source("../analysis/data_prep_sexdiff.R")
source("../analysis/fas.R")
source("../analysis/maha_v8c.R")

```

```{=tex}
\raggedright
\LARGE
```
\textbf{Sex differences in music perception are negligible}

\vspace{0.1in}

```{=tex}
\justifying
\footnotesize
```
Mila Bertolo\*$^{1,2,3,4}$, Daniel Müllensiefen$^{5}$, Isabelle Peretz$^{3}$, Sarah C. Woolley$^{1,2,6}$, Jon T. Sakata†$^{1,2,6}$ & Samuel A. Mehr\*†$^{2,4,7}$

\tiny

^1^Integrated Program in Neuroscience, McGill University, Quebec, QC, Canada H1A 2B4.
^2^Center for Research on Brain, Language, and Music, Quebec, QC, Canada H3G 2A8.
^3^International Laboratory for Brain, Music, and Sound Research (BRAMS), Department of Psychology, University of Montreal, Montreal, QC, Canada.
^4^Yale Child Study Center, Yale University, New Haven, CT 06520, USA.
^5^Department of Psychology, Goldsmiths, University of London.
^6^Department of Biology, McGill University, Quebec, QC, Canada H3A 1B1.
^7^School of Psychology, University of Auckland, Auckland 1010 New Zealand.

†These authors contributed equally.

\*Corresponding authors.
E-mail: \href{mailto:mila.bertolo@mail.mcgill.ca}{mila.bertolo@mail.mcgill.ca}, \href{mailto:sam@auckland.ac.nz}{sam@auckland.ac.nz}

\normalsize
\linenumbers

```{r data load and processing}
min_age <- 7
max_age <- 100
lessons_min_age <- 2
lessons_max_age <- NA #permissible upper age for lessons will be set to current age of participant
use_repeaters <- "No"
exclude_HI <- "Yes"

if(file.exists("../results/dat.Rdata")) {
  load("../results/dat.Rdata")
  } else{
    dat <- data_prep(l.age = min_age, u.age = max_age, lessons.l.age = lessons_min_age, lessons.u.age = lessons_max_age, repeats = use_repeaters, HI = exclude_HI)
  }

n <- nrow(dat)

n_total <- dat %>% summarise(n = n()) %>% pull
n_male <- dat %>% filter(gender_n == "Male") %>% summarise(n = n()) %>% pull()
n_female <- dat %>% filter(gender_n == "Female") %>% summarise(n = n()) %>% pull()
n_other <- dat %>% filter(gender_n == "Other") %>% summarise(n = n()) %>% pull()
n_northamer <- dat %>% 
  filter(country %in% c("Anguilla", "Antigua and Barbuda", "Aruba", "The Bahamas", "Barbados", 
                        "Belize", "Bermuda", "Bonaire", "British Virgin Islands", "Canada", 
                        "Cayman Islands", "Clipperton Island", "Costa Rica", "Cuba", "Curacao", 
                        "Dominica", "Dominican Republic", "El Salvador", "Federal Dependencies of Venezuela", 
                        "Greenland", "Guadeloupe", "Guatemala", "Haiti", "Honduras", "Jamaica", "Martinique", 
                        "Mexico", "Montserrat", "Nicaragua", "Nueva Esparta", "Panama", "Puerto Rico", "Saba",
                        "San Andres and Providencia", "Saint Barthelemy", "Saint Kitts and Nevis", "Saint Lucia", 
                        "Saint Martin", "Saint Pierre and Miquelon", "Saint Vicent and the Grenadines", 
                        "Sint Eustatius", "Sint Maarten", "Trinidad and Tobago", "Turks and Caicos Islands",
                        "United States", "United States Virgin Islands")) %>%
  summarise(n= n()) %>%
  pull()
n_europe <- dat %>%
  filter(country %in% c("Albania", "Andorra", "Armenia", "Austria", "Azerbaijan", "Belarus", "Belgium", 
                        "Bosnia and Herzegovina", "Bulgaria", "Croatia", "Cyprus", "Czech Republic", 
                        "Denmark", "Estonia", "Finland", "France", "Georgia", "Germany", "Greece", 
                        "Hungary", "Iceland", "Ireland", "Italy", "Kazakhstan", "Latvia", "Liechtenstein", 
                        "Luxembourg", "Malta", "Moldova", "Monaco", "Montenegro", "The Netherlands", 
                        "North Macedonia", "Norway", "Poland", "Portugal", "Romania", "Russia",
                        "San Marino", "Serbia", "Slovakia", "Slovenia", "Spain", "Sweden", "Switzerland", 
                        "Turkey", "Ukraine", "United Kingdom", "Vatican City")) %>%
  summarise(n = n()) %>%
  pull()

n_country <- unique(dat$country) %>% length()

require(table1)
dat_gen <- dat %>% filter(gender_n %in% c("Female", "Male"))
table1::table1(~ music_tap_n+out_of_tune_n+music_skill_n+music_listen_n+lessons_enjoy_n+
                 lessons_peers_n+theory_training_n+music_lessons_n+music_feel_n+music_listen_time_n+
                 familiarity_tradmusic_n+lessons_age+music_p_sing_n+education_n+income_n+workspace_n+age+education_n|gender_n,data=dat_gen)

```

```{r Ability factor analysis}
if(file.exists("../results/ef1_ability.RData")){
  load("../results/ef1_ability.RData")
  final_ability_vars <- dimnames(ef1_ability$loadings)[[1]]
  } else{
    ef1_ability <- e.fa(dat[,ability_vars],loading.threshold = min.load, subsample = dat[s1, ability_vars])
    save(ef1_ability,file="../results/ef1_ability.RData")
    final_ability_vars <- dimnames(ef1_ability$loadings)[[1]]
  }
if(file.exists("../results/cf1_ability.RData")){
  load("../results/cf1_ability.RData")
  fm_cf1_ability <- fitMeasures(cf1_ability)
  } else{
    if(is.null(final_ability_vars)) {
      final_ability_vars <- dimnames(e.fa(dat[,ability_vars],loading.threshold = min.load, subsample = dat[s1, ability_vars])$loadings)[[1]]
    }
    cf1_ability <- c.fa(dat[s2, final_ability_vars], check_ordinal=F)
    fm_cf1_ability <- fitMeasures(cf1_ability)
    save(cf1_ability,file="../results/cf1_ability.RData")
  }
if(is.null(dat$mus_ability)){
  dat <- compute_fscores(data=dat, indicator_vars=final_ability_vars, var_name="mus_ability",check_ordinal = F,missing="pairwise")
  save(dat,file="../results/dat.RData")}
```

```{r Self-reported musical ability factor analysis}
if(file.exists("../results/ef1_sfabil.RData")){
  load("../results/ef1_sfabil.RData")
  final_sfabil_vars <- dimnames(ef1_sfabil$loadings)[[1]]
  } else{
    ef1_sfabil <- e.fa(dat[,sf_abililty_vars],loading.threshold = min.load, subsample = dat[s1, sf_abililty_vars])
    save(ef1_sfabil,file="../results/ef1_sfabil.RData")
    final_sfabil_vars <- dimnames(ef1_sfabil$loadings)[[1]]
  }
if(file.exists("../results/cf1_sfabil.RData")){
  load("../results/cf1_sfabil.RData")
  fm_cf1_sfabil <- fitMeasures(cf1_sfabil)
  } else{
    if(is.null(final_sfabil_vars)) {
      final_sfabil_vars <- dimnames(e.fa(dat[,sfabil_vars],loading.threshold = min.load, subsample = dat[s1, sfabil_vars])$loadings)[[1]]
    }
    cf1_sfabil <- c.fa(dat[s2, final_sfabil_vars], check_ordinal=T)
    fm_cf1_sfabil <- fitMeasures(cf1_sfabil)
    save(cf1_sfabil,file="../results/cf1_sfabil.RData")
  }
if(is.null(dat$mus_sfabil)){
  dat <- compute_fscores(data=dat, indicator_vars=final_sfabil_vars, var_name="mus_sfabil",check_ordinal = T,missing="pairwise")
  save(dat,file="../results/dat.RData")
  }
```

```{r Training factor analysis}
if(file.exists("../results/ef1_train.RData")){
  load("../results/ef1_train.RData")
  final_train_vars <- dimnames(ef1_train$loadings)[[1]]
  } else{
    ef1_train <- e.fa(dat[,train_vars],loading.threshold = min.load, subsample = dat[s1, train_vars])
    save(ef1_train,file="../results/ef1_train.RData")
    final_train_vars <- dimnames(ef1_train$loadings)[[1]]
  }
if(file.exists("../results/cf1_train.RData")){
  load("../results/cf1_train.RData")
  fm_cf1_train <- fitMeasures(cf1_train)
  } else{
    if(is.null(final_train_vars)) {
      final_train_vars <- dimnames(e.fa(dat[,train_vars],loading.threshold = min.load, subsample = dat[s1, train_vars])$loadings)[[1]]
      }
    cf1_train <- c.fa(dat[s2, final_train_vars], check_ordinal=T)
    fm_cf1_train <- fitMeasures(cf1_train)
    save(cf1_train,file="../results/cf1_train.RData")
  }
if(is.null(dat$mus_train)){
  dat <- compute_fscores(data=dat, indicator_vars=final_train_vars, var_name="mus_train",check_ordinal = T,missing="pairwise")
  save(dat,file="../results/dat.RData")
  }
```

```{r Listening factor analysis}
if(file.exists("../results/ef1_listen.RData")){
  load("../results/ef1_listen.RData")
  } else{
    ef1_listen <- e.fa(dat[,listen_vars],loading.threshold = min.load, subsample = dat[s1, listen_vars])
    save(ef1_listen,file="../results/ef1_listen.RData")
    final_listen_vars <- dimnames(ef1_listen$loadings)[[1]]
  }
if(file.exists("../results/cf1_listen.RData")){
  load("../results/cf1_listen.RData")
  fm_cf1_listen <- fitMeasures(cf1_listen)
  } else{
    if(is.null(final_listen_vars)) {
      final_listen_vars <- dimnames(e.fa(dat[,listen_vars],loading.threshold = min.load, subsample = dat[s1, listen_vars])$loadings)[[1]]
      }
    cf1_listen <- c.fa(dat[s2, final_listen_vars], check_ordinal=T)
    fm_cf1_listen <- fitMeasures(cf1_listen)
    save(cf1_listen,file="../results/cf1_listen.RData")
  }
if(is.null(dat$mus_listen)){
  dat <- compute_fscores(data=dat, indicator_vars=final_listen_vars, var_name="mus_listen",check_ordinal = T,missing="pairwise")
  save(dat,file="../results/dat.RData")
  }
```

```{r sex differences}
c_dat <- dat
c_dat$gender_n[c_dat$gender_n=="Other"] <- NA
c_dat <- c_dat[!is.na(c_dat$gender_n),]
c_dat$gender_n <- droplevels((c_dat$gender_n))
vars <- c("mpt_ability","mdt_ability","cabat_ability","mus_ability")

if(file.exists("../results/model_coeffs.RData")){
  load("../results/model_coeffs.RData")
  } else{
      c_dat_red <- na.omit(c_dat[,c("gender_n",vars)])
      model_coeffs <- map_dfr(vars, function(v) gamlss(as.formula(sprintf(" %s ~ gender_n", v)), sigma.fo= ~ gender_n,family=NO(sigma.link="identity"),data = c_dat_red, trace=F) %>% broom::tidy())
      model_coeffs <- cbind(DV=rep(vars,each=4),model_coeffs)
      save(model_coeffs, file="../results/model_coeffs.RData")
  }
model_coeffs

# colourblind friendly hex codes from palette.colors(palette = "Okabe-Ito")
# vermillion "#D55E00"  and blue  "#0072B2"

gg_mpt <- ggplot(c_dat, aes(x=mpt_ability, fill=gender_n)) + 
  geom_density(alpha=.3) +
  labs(x = "Mistuning Perception", 
       y = "Density") +
  scale_fill_manual(name="Gender", labels=c("Female", "Male"), values=c("#D55E00", "#0072B2")) +
  theme_classic()
gg_mdt <- ggplot(c_dat, aes(x=mdt_ability, fill=gender_n)) + 
  geom_density(alpha=.3)+
  labs(x = "Melodic Discrimination", 
       y = "Density") + 
  scale_fill_manual(name="Gender", labels=c("Female", "Male"), values=c("#D55E00", "#0072B2")) +
  theme_classic()
gg_cabat <- ggplot(c_dat, aes(x=cabat_ability, fill=gender_n)) + 
  geom_density(alpha=.3) +
  labs(x = "Beat Alignment", 
       y = "Density") + 
  scale_fill_manual(name="Gender", labels=c("Female", "Male"), values=c("#D55E00", "#0072B2")) +
  theme_classic()
gg_mus <- ggplot(c_dat, aes(x=mus_ability, fill=gender_n)) + 
  geom_density(alpha=.3) +
  labs(x = "General Musical Ability", 
       y = "Density") + 
  scale_fill_manual(name="Gender", labels=c("Female", "Male"), values=c("#D55E00", "#0072B2")) +
  theme_classic()

if(file.exists("../results/effectsizes.RData")){
  load("../results/effectsizes.RData")
  } else{
    cohens_ds <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat, pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    overlap <- map_dfr(vars, function(v) p_overlap(as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat, parametric=F, verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    prob_superior <- map_dfr(vars, function(v) p_superiority(as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat, parametric=T, verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    eta_sq <- map_dfr(vars, function(v) etasq(lm(as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat)) %>% broom::tidy())[,3]
    r2 <- map_dfr(vars, function(v) summary(lm(as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat))$r.squared %>% broom::tidy())
    bfs <- map_dfr(vars, function(v) extractBF(ttestBF(formula=as.formula(sprintf(" %s ~ gender_n", v)), data = c_dat))[1]) 
    effectsizes <- data.frame(DV=vars,cohens_d=cohens_ds$mean, pct_overlap=overlap$mean, prob_superior = prob_superior$mean, eta_sq=eta_sq$mean, R2 = r2$x, BayesFactor=bfs$bf)
    
  c_dat_m <- c_dat[c_dat$gender_n=="Male",c("mpt_ability","mdt_ability","cabat_ability")]
  c_dat_f <- c_dat[c_dat$gender_n=="Female",c("mpt_ability","mdt_ability","cabat_ability")]
  mv_es <- maha(c_dat_f, c_dat_m)
  mv_esq <- etasq(lm(cbind(mpt_ability,mdt_ability,cabat_ability) ~ gender_n, data=c_dat))
  effectsizes <- rbind(effectsizes,c("multivariate",mv_es$D,mv_es$OVL, mv_es$CL,mv_esq["gender_n",1], NA))
  save(effectsizes, file="../results/effectsizes.RData")
  }

# for plugging numbers in text
effect_d <- effectsizes %>%
  as.data.frame() %>%
  dplyr::select(DV, cohens_d) %>%
  filter(DV != "multivariate") %>%
  mutate(cohens_d = as.numeric(cohens_d) %>% round(digits = 3))

# variance ratios to plug in text
var_ratio <- c_dat %>%
  dplyr::select(gender_n, mpt_ability, mdt_ability, cabat_ability, mus_ability) %>%
  group_by(gender_n) %>%
  summarise(mpt_var = var(mpt_ability), 
            mdt_var = var(mdt_ability),
            cabat_var = var(cabat_ability),
            mus_var = var(mus_ability)) %>%
  pivot_longer(cols = c("mpt_var", "mdt_var", "cabat_var", "mus_var"),
               names_to = "DV",
               values_to = "variance") %>%
  pivot_wider(names_from = "gender_n", values_from = variance) %>%
  mutate(ratio = round(Male/Female, digits = 3))
```

<!--- summary paragraph, fully referenced 200 words --> 

Since Darwin[@Darwin1871], researchers have proposed that human musicality evolved in a reproductive context in which males produce music to signal their mate quality to females. Sexually selected traits involve tradeoffs in the costs of high-quality signal production and high-fidelity signal detection [@Wiley2006], leading to observable sexual dimorphisms across many species [@Searcy1988; @Irestedt2009]. If musicality is a sexually selected trait in humans, males and females should then differ in their music perception ability, music production ability, or both. The evidence for this possibility is unclear, because previous reports of sex differences in human auditory perception are restricted in scope and inconsistent in direction [@Ravignani2018; @Fitch2006; @Marcus2012; @Bowling2021; @Zentner2021; @Wolf2018; @Sluming2000; @Borniger2013; @Miles2016; @Wisniewski2014; @Mosing2015]. Here, we report a test of music processing ability in `r n_male` men and `r n_female` women from `r n_country` countries. In contrast to other non-musical human traits [@Archer2019; @Lippa2009; @Puts2011; @Archer2004], and in contrast to music-related traits in non-human animals [@Hoeschele2012; @Hahn2017; @Rouse2023; @Christie2004], we found no consistent advantage for either sex. The sex differences we did observe were negligible (Cohen's \textit{d} range: `r abs(effect_d$cohens_d) %>% min()`-`r abs(effect_d$cohens_d) %>% max()`) and Bayesian analyses indicated evidence in favor of the null hypothesis of no sex difference in general musical ability (Bayes Factor = 0.6). These results suggest that it is unlikely that music evolved in the context of sexual selection.

## Main Text

Music is ubiquitous across human societies, and there are numerous similarities in the forms and functions of musical styles across societies and cultures [@Mehr2019; @Yurdum2022; @Lomax1977; @Singh2023; @Jacoby2021; @Anglada-Tort2023; @Sievers2013; @Fritz2009; @Trehub1993].
The biological and evolutionary roots of musicality, accordingly, have been debated at length [@Mithen2005; @Fitch2006b; @Honing2018a; @Ravignani2018; @Marcus2012; @Pinker1997; @Orians2014; @Honing2018b]. One common hypothesis, originally proposed by Darwin [@Darwin1871], but endorsed by many others [@Werner1997; @VanDenBroek2009; @Miller2000; @Madison2018; @Novaes2023] is that music evolved via sexual selection as a credible signal of mate quality: males produce music to signal mate quality to females, while females assess potential mates based on their musical ability. This idea echoes credible signals of mate quality found in many non-human species, such as in the tungara frog (\textit{Physalaemus pustulosus}), where females select males based on the quality of advertisement calls [@Ryan2019]; in birds-of-paradise (\textit{Paradisaeidae}), where males evolved highly complex behavioral, morphological, and acoustic "courtship phenotypes" [@Ligon2018]; and in red-winged black birds (\textit{Agelaius phoeniceus}), where females but not males display acute differentiation between male calls and other species' imitations of them [@Searcy1988]. 

Sexually selected traits demonstrate a reliable pattern of sexual dimorphism because the costs involved with signal production and detection differ across the sexes. In species in which females incur the larger cost for reproduction (e.g., larger gametes, internal fertilization, pregnancy, and lactation), females tend to be more discriminating of social signals such as courtship vocalizations because the cost of reproducing with a lower-quality mate is higher for females [@Green1966; @Trivers1972; @Searcy1988; @Dabelsteen1993; @Otter1997; @Wiley2006]. Indeed, female advantages in the sensory detection and discrimination of courtship signals have been documented in bird [@Hoeschele2012; @Hahn2017], amphibian [@Bernal2007], and mammalian species [@Krizman2021]. If music evolved in the context of sexual selection, human males and females should therefore differ in their music perception ability, music production ability, or both. 

The evidence for this prediction is mixed [@Ravignani2018; @Fitch2006; @Marcus2012; @Mehr2021; @Bowling2021; @Zentner2021]. Some studies report higher musical abilities in men  [@Wolf2018] and in individuals with lower digit ratios (a trait taken to indicate higher in-utero testosterone exposure)[@Sluming2000]. Others report a female advantage in melodic recognition [@Miles2016] and auditory sensitivity [@Wisniewski2014]. Still others report no sex difference in pitch or beat perception abilities overall but possible differences in prevalence at the low extremes of these abilities [@Peretz2017]. Two studies of associated predictions of a sexual selection account have had mixed results: testosterone levels, as measured via saliva assays, were not predictive of men's musical aptitude [@Borniger2013], and in a large twin study, higher musical ability correlated with \textit{lower} reproductive success [@Mosing2015].

Further, several papers reporting evidence of sexual selection for musicality have been retracted or have documented failures to replicate: the claim that women are more attracted to men with apparent musical ability [@Gueguen2014] was retracted in 2020 [@Gueguen2021]; the claim that women are more attracted to higher quality dancing in men [@Brown2005], retracted in 2013 [@Brown2013a]; and the claim that women prefer more complex music around ovulation [@Charlton2012] failed to replicate [@Charlton2014]. Moreover, even in cases where a \textit{bona fide} sex difference in found, such a difference could reflect sociocultural forces rather than biological forces [@Marcus2012]. This pattern has led to some skepticism regarding the sexual selection hypothesis for musicality evolution [@Marcus2012; @Ravignani2018; @Mehr2021].

Here, we employ a large-scale, citizen-science approach to examine sex differences in music perception. `r n_male + n_female` people participants from `r n_country` countries (see \textit{Figure 1}) were recruited via https://themusiclab.org; of these, $n =$ `r n_female`  (`r round((n_female/n_total)*100, digits = 1)`%) self-identified as "female", and  $n =$ `r n_male` (`r round((n_male/n_total)*100, digits = 1)`%) self-identified as "male". They completed three musical perception-based tests presented in a random order, measuring their i) mistuning perception [@Larrouy-Maestri2019], ii) melodic detection [@Harrison2017], and iii) beat alignment perception [@Harrison2018]. They also reported demographic information and details of their musical training (if any) and listening habits (see \textit{SI Table 2}). 

```{r mega-fig, results= "asis", fig.pos = "H", fig.width = 9, fig.height = 9, fig.cap="\\textbf{Fig. 1 | In a large global sample of participants, we find negligibly small sex differences in general musical ability. A} Geographic spread of participants who completed music perception tests. The shading of each country indicates the number of participants who self-reported that country as their location. \\textbf{B} In an aggregate measure of general musical ability (composed of measures \\textbf{C, D, E}), there is a negligibly small difference between men and women's scores. The shaded areas represent kernel density estimations."}
listener_map <- readPNG("../viz/DIFF_listeners_manuallycleaned.png")
listener_map <- ggdraw() + draw_image(listener_map)

gg_mus_megafig <- gg_mus + 
  theme(legend.justification=c(0,1), legend.position=c(0,1))
gg_mpt_megafig <- gg_mpt +
  theme(legend.position = "none")
gg_mdt_megafig <- gg_mdt +
  theme(legend.position = "none")
gg_cabat_megafig <- gg_cabat +
  theme(legend.position = "none")

three_tests <- plot_grid(ggarrange(gg_mpt_megafig, gg_mdt_megafig, gg_cabat_megafig, ncol= 1, nrow=3,  common.legend = TRUE, legend="none", labels = c("C", "D", "E")), label_size = 12, ncol = 1) 

gg_all <- plot_grid(gg_mus_megafig, three_tests, ncol = 2, rel_widths = c(2/3, 1/3), labels = c("B"))

plot_grid(listener_map, gg_all, ncol = 1, nrow = 2, rel_heights = c(0.4, 0.6), labels = c("A"))
```

```{r mean differences}
mean_diffs <- c_dat %>%
  dplyr::select(gender_n, mpt_ability, mdt_ability, cabat_ability, mus_ability) %>%
  group_by(gender_n) %>%
  summarise(mpt_mean = mean(mpt_ability), 
            mpt_sd = sd(mpt_ability), 
            mdt_mean = mean(mdt_ability),
            mdt_sd = sd(mdt_ability), 
            cabat_mean = mean(cabat_ability), 
            cabat_sd = sd(cabat_ability),
            mus_mean = mean(mus_ability),
            mus_sd = sd(mus_ability)) %>%
  mutate(across(where(is.numeric), ~round(., digits = 3)))
```

### Are there sex differences in music perception?

We found statistically significant but negligibly sized sex differences in a measure of general musical ability derived from the three tests, along with similarly negligible differences on each of the tests (p \< 0.05 for each; \textit{Figure 1, Table 1}).
Women, on average, scored higher than men on general musical ability, but barely so (mean $\pm$ SD; women: `r mean_diffs[mean_diffs$gender_n == "Female", "mus_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Female", "mus_sd"]`; men: `r mean_diffs[mean_diffs$gender_n == "Male", "mus_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Male", "mus_sd"]`). This difference is approximately 200 times smaller than the size of the average height difference between human males and females, a well-documented sexual dimorphism ($d = 1.63$ [@Lippa2009]). Further, a Bayesian approach to this analysis provided evidence leaning in favor of the null hypothesis of no true sex difference in general musical ability (Bayes Factor = 0.6; \textit{Table 1}).

The pattern of results on the individual perception tests was internally inconsistent. Women outperformed men in the mistuning perception test (women; `r mean_diffs[mean_diffs$gender_n == "Female", "mpt_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Female", "mpt_sd"]`; men: `r mean_diffs[mean_diffs$gender_n == "Male", "mpt_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Male", "mpt_sd"]`), but men outperformed women on the beat alignment (women: m = `r mean_diffs[mean_diffs$gender_n == "Female", "cabat_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Female", "cabat_sd"]`; men: `r mean_diffs[mean_diffs$gender_n == "Male", "cabat_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Male", "cabat_sd"]`) and melodic discrimination tests (women: `r mean_diffs[mean_diffs$gender_n == "Female", "mdt_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Female", "mdt_sd"]`; men: `r mean_diffs[mean_diffs$gender_n == "Male", "mdt_mean"]` $\pm$ `r mean_diffs[mean_diffs$gender_n == "Male", "mdt_sd"]`). Bayesian approaches to these differences supported their robustness (Bayes Factors >1000), despite their very small sizes and inconsistent directions. Cohen's \textit{d} effect sizes ranged from `r abs(effect_d$cohens_d) %>% min()`-`r abs(effect_d$cohens_d) %>% max()`, and all of these values would be considered as null or very small effects [@Hyde2014; @Archer2019; @Zell2015] (see \textit{Table 1} for other measures of effect sizes). 

```{r cohend_self_assessed_skill}
# cohen d effsizes between self-assessed music ability groups
# between (1) "no skill at all" and (2) "novice"
cohens_ds_musskill_12 <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(1,2)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
# between (2) "novice" and (3) "some skill"
cohens_ds_musskill_23 <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(2,3)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
# between (3) "some skill" and (4) "a lot of skill"
cohens_ds_musskill_34 <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(3,4)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
# between (4) "a lot of skill" and (5) "expert"
cohens_ds_musskill_45 <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(4,5)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
# between (1) "no skill at all" and (5) "expert"
cohens_ds_musskill_15 <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(1,5)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]


# partial eta squares of gender and self-assessed musical skill, for plugging in text
mv_esq <- etasq(lm(cbind(mpt_ability,mdt_ability,cabat_ability) ~ gender_n, data=c_dat))
mv_esq_musskill <- etasq(lm(cbind(mpt_ability,mdt_ability,cabat_ability) ~ music_skill_n, data=c_dat))

# for women
# between (1) "no skill at all" and (5) "expert"
cohens_ds_musskill_15_women <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(1,5) & gender_n == "Female"), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]

# for men
# between (1) "no skill at all" and (5) "expert"
cohens_ds_musskill_15_men <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_skill_n", v)), data = filter(c_dat, music_skill_n %in% c(1,5) & gender_n == "Male"), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]

# difference between those who had and had not taken music lessons 
cohens_d_music_lessons <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ music_lessons_n", v)), data = filter(c_dat, music_lessons_n %in% c(0,1)), pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
```

```{r fig3, results = 'asis', fig.show="hold", out.width="100%", fig.cap = paste("\\textbf{Fig. 2 | Distributions of general musical ability scores, by self-reported musical skill.} As expected, general musical ability scores differ much more along other participant covaraites, such as self-estimated musical skill, than they differ by sex. The shaded areas represent kernel density estimations. The verbal prompt was \\textit{`Think of your skill at making music (using a musical instrument or singing). How would you rate your own skill?'} The participant covariate of self-assessed musical skill captured much more variance in participants' general musical ability scores, compared to their sex (partial $\\eta^2 =$", round(mv_esq_musskill, digits = 3), " for self-assessed musical skill, relative to partial $\\eta^2 =$", round(mv_esq, digits = 3), " for sex). Cohen's \\textit{d} values were all in the range of what would conventionally be described as medium effect sizes (Cohen's \\textit{d} between `no skill' and `novice' = ", cohens_ds_musskill_12[4,]$mean %>% abs() %>% round(digits = 3), "; between `novice' and `some skill' =", cohens_ds_musskill_23[4,]$mean %>% abs() %>% round(digits = 3), "; between `some skill' and `a lot of skill' =", cohens_ds_musskill_34[4,]$mean %>% abs() %>% round(digits = 3), "; between `a lot of skill' and `expert' =", cohens_ds_musskill_45[4,]$mean %>% abs() %>% round(digits = 3), ").", sep = "")}

c_dat %>%
  mutate(music_skill = as.factor(music_skill_n)) %>%
  filter(!is.na(music_skill)) %>%
  ggplot(aes(x=mus_ability, fill = music_skill)) +
  geom_density(alpha=.3) +
  labs(x = "General Musical Ability") +
  scale_fill_manual(name="Self-assessed Music Skill", labels=c("I have no skill at all", "I\'m a novice", "I have some skill", "I have a lot of skill", "I\'m an expert"), values = c("#E69F00", "#F0E442", "#009E73", "#56B4E9", "#CC79A7")) +
  theme_classic() + 
  theme(legend.justification=c(0,1), legend.position=c(0,1))
```

This lack of clear sex differences cannot be explained by methodological quirks or poor testing; multiple other participant measures explained substantial variability in general musical ability scores. For example, participants' self-assessed music production skill level was strongly related to performance on the music perception tests: those reporting they "have no skill" performed far worse than those reporting being "experts" ($d =$ `r signif(cohens_ds_musskill_15[4,], 3) %>% abs()`; \textit{Figure 2}). This relation also held when analyzing men and women separately; in women, the difference in general musical ability between "no skill" and "expert" was $d =$ `r signif(cohens_ds_musskill_15_women[4,], 3) %>% abs()`, and in men it was $d =$ `r signif(cohens_ds_musskill_15_men[4,], 3) %>% abs()` (see \textit{SI Figure 2}). Similarly, the degree of self-reported music lessons was highly predictive of performance of the tests; those participants who reported they had taken lessons scored higher than those who reported having never taken lessons ($d =$ `r signif(cohens_d_music_lessons[4,], 2) %>% abs()`; \textit{SI Figure 3}); and moderately sized effects of language experience on music perception ability in a subset of these data have been reported elsewhere [@Liu2023].

### Is there Greater Male Variability?
The "Greater Male Variability Hypothesis" argues that some dimorphisms manifest as greater variability - not greater averages - in males than in females (e.g., "more idiots, more geniuses"[@Pinker1997]). Significant (p \< 0.05 for each) but small gender differences in variances were observed, but like the effects found in individual tests, they were of internally inconsistent directions: variances were significantly larger for men in the mistuning perception (men:women variance ratio: `r var_ratio[var_ratio$DV == "mpt_var", "ratio"]`) and melodic discrimination tests (men:women variance ratio: `r var_ratio[var_ratio$DV == "mdt_var", "ratio"]`), and larger for women in the beat alignment test (men:women variance ratio: `r var_ratio[var_ratio$DV == "cabat_var", "ratio"]`; \textit{Table 3}). The variance ratios all being close to 1, however, indicates highly similar variances between men and women [@Hyde2014]. Thus, we find no clear evidence in support of higher male variability.

```{r fig2, eval = FALSE, out.width="100%", fig.cap="\\textbf{Fig. X | Distributions of general musical ability scores for men and women} Statistically significant but miniscule differences were found between men and women. Women, on average, scored higher on general musical ability and mistuning perception. Men, on average, scored higher on beat alignment and melodic discrimination. All effect sizes ranged from 0.009 - 0.112; these effect sizes are typically considered negligeable or very small."}
ggarrange(gg_mpt, gg_mdt, gg_cabat, gg_mus, ncol=2, nrow=2, common.legend = TRUE, legend="bottom")
```

```{r table2, results = "asis"}
# renaming var_ratio so it can be joined to effectsizes
var_ratio_tojoin <- var_ratio %>%
  mutate(DV = ifelse(DV == "mpt_var", "mpt_ability", 
                      ifelse(DV == "mdt_var", "mdt_ability", 
                             ifelse(DV == "cabat_var", "cabat_ability", "mus_ability")))) %>%
  dplyr::select(DV, ratio)

effectsizes %>%
  filter(DV != "multivariate") %>%
  # merge in variance ratios 
  left_join(var_ratio_tojoin) %>%
  #round all values
  mutate(across(c(-DV), as.numeric)) %>%
  mutate(across(c(eta_sq, R2), round, 5)) %>% # these two rounded to 4 decimal points to show not = 0
  mutate(across(c(cohens_d, pct_overlap, prob_superior, BayesFactor), round, 3)) %>%
  mutate(across(c(BayesFactor), formatC, format = "e", digits = 2)) %>% # scientific notation
  # rename variable names
  mutate(DV = ifelse(DV == "mpt_ability", "Mistuning Perception", 
                     ifelse(DV == "mdt_ability", "Melodic Discrimination", 
                            ifelse(DV == "cabat_ability", "Beat Alignment", 
                                   "General Music Ability")))) %>%
  kable(.,
        format = "latex",
        booktabs = TRUE,
        escape = F,
        col.names = c("Task", "Cohen's D", "\\% Overlap", "Probability Superior", "$\\eta^2$", "$R^2$", "Bayes Factor", "Variance Ratio (m/f)")) %>%
        footnote(general = "Effect sizes of gender differences in musical listening tasks. Note: Negative Cohen's D values indicate that men's scores are higher, and positive scores that women's scores are higher.",
           general_title = "Table 1 | Effect Sizes",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  column_spec(column = 2:8, width = "0.55in") %>%
  kable_styling(position = "center", 
                latex_options = "hold_position") 
```


```{r effect sizes with covariates, results = "asis"}
p_esq <-  map_dfr(vars, function(v) eta_squared(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))[1,"Eta2_partial"] %>% broom::tidy())
r2 <- map_dfr(vars, function(v) summary(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))$r.squared %>% broom::tidy())


out <- data.frame(DV=vars, p_eta_sq_gender=p_esq$x, R2_full_model=r2$x)
# 
# table 3 in SI
# out %>%
#   mutate(DV = ifelse(DV == "mpt_ability", "Mistuning Perception", 
#                      ifelse(DV == "mdt_ability", "Melodic Discrimination", 
#                             ifelse(DV == "cabat_ability", "Beat Alignment", "General Musical Ability")))) %>%
#   mutate(p_eta_sq_gender = round(p_eta_sq_gender, digits = 3), 
#          R2_full_model = round(R2_full_model, digits = 3)) %>%
#   kable(.,
#         format = "latex",
#         booktabs = TRUE,
#         col.names = c("Task", "$\\eta^2$ of gender", "${R}^2$ full model")) %>%
#         footnote(general = "The estimated gender effect remains of a comparable magnitude when other participant covariates - age, education, age at start of any music lessons, musical training, and musical listening - are added.",
#            general_title = "Table 3. Full Model, gender and covariates",
#            footnote_as_chunk = TRUE,
#            title_format = "bold",
#            escape = FALSE,
#            threeparttable = TRUE) %>%
#   kable_styling(position = "center", 
#                 latex_options = "HOLD_position")
```

### How well do other factors drive music perception ability?
We asked whether performance on the music perception tests might have been driven by participant covariates beyond their sex, by repeating the main analyses while adjusting for participants' age, education, age of start of music lessons (for participants who had completed some music lessons), and factors indicative of their musical training and music listening habits (for details about the factors, see \textit{Analysis, \& Table 3}). The inclusion of these covariates did not substantially alter effect sizes of sex differences (\textit{SI Table 2}). Further analyses using structural equations modelling approaches also detected minuscule sex differences on the tests; these too found little influence of covariates on the magnitude of our estimated sex differences (see \textit{Analysis \& SI Figure 1}).

### Discussion
Together, these findings provide no evidence for substantive sex differences in human music perception. Not only did the most general test of sex differences only find weak evidence for a sex difference (i.e. a statistically significant differences with a negligible effect size, and a low Bayes Factor of 0.6), but the reliable sex differences we \textit{did} find were of a very small size and of inconsistent direction. 

Those few reliable differences do not seem to support any claim of sexual selection, for two main reasons. First, human sex differences in traits argues to result from sexual selection tend to be large in size; some examples include height (d = 1.63)[@Lippa2009], vocal acoustic features (d = 2.7 - 5.7)[@Puts2011], and rates of engagement in physical aggression (d = 1.11)[@Archer2019]. These effects dwarf the sex differences we found on the three music tests.

Second, large sex differences have been observed in a variety of non-human animals that \textit{do} use acoustic signals for courtship and sexual behavior. Female black-capped chickadees (\textit{Poecile atricapillus}) outperform males on the discrimination of pitch ratios (partial $\eta2=$ 0.163) [@Hoeschele2012], and are significantly faster than males at discriminating between socially dominant and subordinate male songs (Cohen's \textit{d} = 2.854) [@Hahn2017]; and male Bengalese finches do not show the same individual recognition of other males (as seen in heart rate habituation) that females do [@Ikebuchi2003]. Because in these species males and females often have categorically different behavioral responses to song, studies directly comparable to our own - where both males and females are presented with the same stimuli and assessed on the same behavioral assay - are sparse in the literature, and so there is little consensus as to whether these sex differences are strictly perceptual and/or motivational. While the exact mechanisms underlying these sex differences in non-humans animals' responses to these courtship signals is unclear, the magnitude of sex difference in behavioral responses equally dwarf the effects we found on the three music tests.

The lack of sex differences in music perception casts doubt on the mate quality hypothesis, but does not rule it out entirely. The ability to produce music requires complex sensory processing and sensorimotor integration; one can imagine unusual co-evolutionary dynamics at work, where music production is substantively dimorphic but music perception is not. We find this unlikely: in our data, self-reported music production skill was highly correlated with music perception ability in both males and females, suggesting the negligible sex difference in music perception extends to a comparable equivalence in music production abilities. While self-reports of music production ability are coarse measures relative to structured assessments of music production ability, we predict that future research using such tests will find comparably poor evidence for sexual dimorphism in musicality. 

\bigskip

# Methods

This research was approved by the Committee on the Use of Human Subjects and Harvard University's Institutional Review Board (protocol 2000033433). Data was collected from an online experiment hosted at \url{https://www.themusiclab.org/quizzes/miq} and advertised as a "Test Your Musical IQ" game. Recruitment was driven mainly by organic social media sharing which advertised the game to our global participant pool.

## Participants

Our publicly accessible and globally disseminated online test reached `r n_total` participants from `r n_country` countries (at time of writing, where we considered data gathered between 22 Nov 2019 and 14 Dec 2020).
Included in this dataset are only individuals who self-reported that they had not played the game before or did not have a hearing impairment.
Of these, n = `r n_female` (`r round((n_female/n_total)*100, digits = 1)`%) self-identified as "female", n = `r n_male` (`r round((n_male/n_total)*100, digits = 1)`%) self-identified as "male" and n = `r n_other` (`r round((n_other/n_total)*100, digits = 1)`%) indicated "other" for their gender.
Because sexual selection theory does not make explicit predictions with regards to how non-binary participants may differ from men and women, we only used data from participants who self-identified as either male or female.
While individuals from over 200 countries participated in the test, the majority of participants were from North America (`r n_northamer`, or `r round((n_northamer/n_total), digits = 3)*100`%) and Europe (`r n_europe`, or `r round((n_europe/n_total), digits = 3)*100`%).
Further information about participants' age and educational achievement is detailed in Table 2 and SI Table 1.

```{r table1 participants, results = "asis"}
dat %>%
  filter(gender_n %in% c("Male", "Female")) %>%
  group_by(gender_n) %>% 
  summarise(N = n(),
            age_mean = round(mean(age), digits = 1), 
            # there may be a more elegant way to do this with across()
#            music_tap_n = mean(music_tap_n, na.rm = TRUE),
            music_lessons_n = round(mean(music_lessons_n, na.rm = TRUE), digits = 2)*100,
            music_lessons_n = paste0(music_lessons_n, "\\%"),
            lessons_age = round(mean(lessons_age, na.rm = TRUE), digits = 1),
            theory_training_n = round(mean(theory_training_n, na.rm = TRUE), digits = 1),
            familiarity_tradmusic_n = round(mean(familiarity_tradmusic_n, na.rm = TRUE), digits = 1)) %>%
  janitor::clean_names() %>%
  kable(.,
        format.args = list(big.mark = ","), # 1000 comma separator 
        format = "latex",
        booktabs = TRUE,
        col.names = c("Gender", "n", "Mean Age (years)", "Music Lessons", "Age Music Lessons Started", "Music Theory Training", "Familiarity World Music"),
        escape = FALSE, 
        linesep = "\\addlinespace",
        longtable = TRUE,
        align=rep('l', 10))  %>%
        footnote(general = "Participant information, broken down by self-reported gender, age, and music lessons. Note for Music Theory Training: answers ranged from `No music theory traning.' (scored 0) to `A lot of music theory training' (scored 4). Note for Familiarity Traditional Music: answers ranged from `I have never heard traditional music' (scored 0) to `I am very familiar with traditional music' (scored 4).",
           general_title = "Table 2 | Participants",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  kable_styling(full_width = TRUE) #, latex_options = "HOLD_position"
```


## Measures of music perceptual abilities

Participants were given three rounds of musical perception-based tests, each of which were based on existing and pre-validated music perception tests.
These tests measures participants' abilities in the following domains:\

i)  Melodic discrimination [@Harrison2017]: In this test, participants listen to three different versions of the same melody.
    All three versions are played at different absolute pitch levels, with subsequent versions always transposed a semitone higher compared to the previous version.
    Two versions are identical in their interval structure but one of the three versions differs in one note.
    Differences can be in melodic interval, contour or tonality or any combination of the three factors.
    The participant's task is to identify the odd-one-out.
    The test is based on an explanatory item response theory model and is adaptive (for details see Harrison et al. (2017) [@Harrison2017]).
    This allows the procedure to select melody items dynamically such that item difficulty is matched to estimated participant ability for each trial.
    Final participant scores are computed according to the underlying item response theory model and have a theoretical range from -4 to 4, where higher scores represent greater melodic discrimination ability.
    All three music perception tests are adaptive in this way, with participant scores computed as described above.\

ii) Mistuning perception [@Larrouy-Maestri2019]: In this test, participants are given two nearly identical short excerpts of vocal music.
    In one excerpt the vocal track was pitch-shifted against the instrumental backtrack; the participant's task is to indicate in which of the two excerpts the singer sounds more out of tune.\

iii) Beat alignment [@Harrison2018]: In this test, participants listen to the same clip of music twice, with an overlaid beep track.
     In one version the beep track is perfectly aligned with the musical beat of the music, and in the other version the beep track is slightly shifted in time.
     The participant's task is to identify in which of the two clips the beep track is best aligned with the beat of the music.

The short excerpts of music were all excerpts of Western popular music, i.e., music in a style that would be familiar to listeners acculturated to Western music, but not songs that they were likely to have heard before; all stimulus materials were sourced from stock libraries containing only songs that have not been commercially released. While our tests consisted only of musical excerpts in the Western musical tradition, because men and women from all geographic regions participated in the study, this test remains useful for quantifying sex differences around the world; any culture-driven differences in performance would apply equally to both men and women, and therefore are unlikely to confound the presently reported findings. 

## Procedure

Participants completed the experiment on computers, smartphones, or tablets. Participants were first asked to report their age, country, self-identified gender, native language, and whether they had a hearing impairment. They were also asked about the noisiness of the environment in which they were at the time of the experiment, whether they were wearing headphones, and if they had played the game before. Participants then completed the three music perception tests, in a randomized order. After the tests, they were asked a number of questions concerning their musical skill and experience (see \textit{SI Table 3}). 

## Analysis

### Data pre-processing

Of the \>2 million people who had begun the online experiment at https://www.themusiclab.org/miq, we considered only those who had completed all three musical perception tests, only analyzed the data from participants who self-reportedly had not completed this game before, and excluded participants who reported an age younger than 7 or older than 100. Applying these filters resulted in a participant pool of n = `r n_total`.
Questions that had written response labels were re-coded into binary or ordinal variables (see \textit{SI Table 3});
these items formed conceptually meaningful groups: items pertaining to \textit{i)} musical perception ability (ie. the three musical perception tests), \textit{ii)} self-reported musical ability, \textit{iii)} formal musical training, and \textit{iv)} extent, intensity, and breadth of musical listening. 

### Factor analysis to extract composite scores

For each of the above groups of measures, information from their constituent sources was aggregated by factor analysis.
For each of these factor analyses, the data sample was randomly split into two equal sized subsamples.
An exploratory minimum residual factor analysis was run on one subsample, with the correlation matrix of the variables belonging to the same conceptual group.
Questions differed in their response options and, by extension, the measurement level of corresponding variables.
In these cases a mixed-type correlation matrix was computed employing polychoric, tetrachoric, biserial, point-biserial or Pearson correlations as appropriate for each pair of questions.
The resulting correlation matrix was then subjected to a factor analysis.
In all cases only a single factor was specified and only variables that loaded with a minimum of 0.3 were retained in the factor model.
The second subsample was used for a confirmatory factor analysis and the factor structure was confirmed by inspecting measures of absolute model fit.
On the condition that fit measures were satisfactory, the confirmatory factor model was then computed a second time, using the entire sample of participants, and factor scores were computed using only data from those participants with complete data on all the variables included in the factor model.
We then used the Empirical Bayes Modal approach for computing factor scores from the confirmatory model.
Table 3 details each factor's individual items, their loading on the factor, and how much variance among the indicators is explained by the single factor.
For all factors, the confirmatory fit measures indicated a good model fit (General musical ability RMSEA \< .001, SRMR, \< .001; self-reported musical ability RMSEA = 0.029, SRMR = 0.037; self-reported musical training RMSEA \< .001, SRMR, \< .001; self-reported musical listening RMSEA \< .001, SRMR, \< .001).

```{r table factor analysis, results= "asis"}
fa_ability <- stack(ef1_ability$loadings[1:3,]) %>% 
  mutate(values = round(values, digits = 3)) %>%
  mutate(factor = "Music ability") %>%
  mutate(ind = ifelse(ind == "mpt_ability", "Mistuning Perception", 
                      ifelse(ind == "mdt_ability", "Melodic Discrimination", 
                             "Beat Alignment"))) %>%
  mutate(var_explained = round(ef1_ability$Vaccounted[2], digits = 3))
fa_sfabil <- stack(ef1_sfabil$loadings[1:4,]) %>% 
  mutate(values = round(values, digits = 3)) %>%
  mutate(factor = "Self-reported musical ability") %>%
  mutate(ind = ifelse(ind == "music_tap_n", "Tap In Time", 
                      ifelse(ind == "out_of_tune_n", "Out of Tune", 
                             ifelse(ind == "music_skill_n", "Musical Skill", 
                                    "Music Listening Skill")))) %>%
  mutate(var_explained = round(ef1_sfabil$Vaccounted[2], digits = 3))
fa_train <- stack(ef1_train$loadings[1:3,]) %>% 
  mutate(values = round(values, digits = 3)) %>%
  mutate(factor = "Musical Training") %>%
  mutate(ind = ifelse(ind == "lessons_enjoy_n", "Enjoyed Lessons", 
                      ifelse(ind == "lessons_peers_n", "Compared to Peers in Lessons", 
                             "Theory Training"))) %>%
  mutate(var_explained = round(ef1_train$Vaccounted[2], digits = 3))
fa_listen <- stack(ef1_listen$loadings[1:3,]) %>% 
  mutate(values = round(values, digits = 3)) %>%
  mutate(factor = "Musical Listening") %>%
  mutate(ind = ifelse(ind == "music_feel_n", "Music Enjoyment", 
                      ifelse(ind == "music_listen_time_n", "Music Listening Time", 
                             "Familiarity with World Music"))) %>%
  mutate(var_explained = round(ef1_listen$Vaccounted[2], digits = 3))

rbind(fa_ability, fa_sfabil, fa_train, fa_listen) %>%
  dplyr::select(factor, ind, values, var_explained) %>%
  dplyr::rename(item = ind, loading = values) %>%
  janitor::clean_names() %>%
  kable(.,
        format = "latex",
        booktabs = TRUE,
        col.names = c("Factor", "Item", "Loading onto Factor", "Variance Explained"),
        escape = FALSE, 
        linesep = "\\addlinespace",
        longtable = TRUE,
        align=rep('l', 10))  %>%
        footnote(general = "Aggregation of related items into factors, and individual items' loadings onto their factor.",
           general_title = "Table 3 | Factor Analysis",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  collapse_rows() #%>%
#  kable_styling(full_width = TRUE, 
#                latex_options = "HOLD_position")
```

### Estimating sex differences in music perception 

To test whether there were any sex differences in music perception, after controlling for the above participant covariates, we tested for sex differences using a sequence of different analysis approaches.

First, a general additive model (GAM) was fit to each dependent variable, i.e., the three individual ability tests (melodic discrimination, mistuning perception, and beat alignment), and the aggregate factor score of general musical ability. This allowed us to calculate means and variances of these variables and test whether they differed between men and women. 
Men and women's distributions on each variable were plotted to visualize the extent of their overlap (\textit{Figure 1}). On the basis of these data, we also calculated the probability superior (the probability that a randomly selected woman would have a higher score than a randomly selected man), the $\eta^2$ of how much variance sex explains in the models, the ${R}^2$ of the models, and the variance ratios (variance men / variance women) (\textit{Table 1}).
As a measure of effect size of a potential sex difference, we calculated Cohen's \textit{d} as a parametric effect size, and percentage overlap of the two distributions as a non-parametric effect size (\textit{Table 1}).  

```{r Covariate adjustement, cache = TRUE}
# MB: temporarily setting eval = FALSE so I can quickly render doc 
p_esq <-  map_dfr(vars, function(v) eta_squared(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))[1,"Eta2_partial"] %>% broom::tidy())
r2 <- map_dfr(vars, function(v) summary(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))$r.squared %>% broom::tidy())

# out <- data.frame(DV=vars, p_eta_sq_gender=p_esq$x, R2_full_model=r2$x)
# print(out)
```

```{r Latent variable difference via SEM, cache = TRUE}
# Compare SEMs without covariates 
m0 <- '
  g =~ mpt_ability + mdt_ability + cabat_ability
  mpt_ability ~ c(0,0)*1
  
  g ~ c(int_female,int_male)*1
  diff:= int_female - int_male
'

lv_sex_diffs <- cfa(m0,data=c_dat,group = "gender_n", missing ="FIML", se="robust",group.equal=c("loadings","intercepts"))

pars_0 <- parTable(lv_sex_diffs)
lv_diff_0 <- pars_0[pars_0$lhs=="diff","est"] 
n_f <- lavInspect(lv_sex_diffs,"nobs")[1]
n_m <- lavInspect(lv_sex_diffs,"nobs")[2]
var_g_f <- lavInspect(lv_sex_diffs, "cov.lv")$Female
var_g_m <- lavInspect(lv_sex_diffs, "cov.lv")$Male
S_0 <- sqrt(((n_f-1)*var_g_f+(n_m-1)*var_g_m)/(n_f+n_m+2))

cohensd_lv_sex_diffs <- as.numeric(lv_diff_0 / S_0)

#summary(lv_sex_diffs,fit.measures=T, standardized=T)

# layout(t(1:2))
# semPaths(lv_sex_diffs,intercepts=T, "eq", ask=F, intAtSide = T,"std",mar = c(8, 1, 5, 1))

# Compare SEMs with covariates 
m1 <- '
  g =~ mpt_ability + mdt_ability + cabat_ability
  mpt_ability ~ c(0,0)*1
  g ~ mus_train + age + education_n +  mus_listen + lessons_age_n
  
  g ~ c(int_female,int_male)*1
  diff:= int_female - int_male
'

lv_sex_diffs_covariates <- sem(m1,data=c_dat,group = "gender_n", missing ="FIML",group.equal=c("loadings","intercepts"))

pars_1 <- parTable(lv_sex_diffs_covariates)
lv_diff_1 <- pars_1[pars_1$lhs=="diff","est"] 
n_f <- lavInspect(lv_sex_diffs_covariates,"nobs")[1]
n_m <- lavInspect(lv_sex_diffs_covariates,"nobs")[2]
var_g_f <- lavInspect(lv_sex_diffs_covariates, "cov.lv")$Female
var_g_m <- lavInspect(lv_sex_diffs_covariates, "cov.lv")$Male
S_1 <- sqrt(((n_f-1)*var_g_f+(n_m-1)*var_g_m)/(n_f+n_m+2))

cohensd_lv_sex_diffs_covariates <- as.numeric(lv_diff_1 / S_1)

#summary(lv_sex_diffs_covariates,fit.measures=T, standardized=T)

# layout(t(1:2))
# semPaths(lv_sex_diffs_covariates,intercepts=T, "eq", ask=F, intAtSide = T, "est",mar = c(8, 1, 5, 1))

```

Secondly, in order to further test whether men and women differed on this latent variable "general musical ability" that we purport to measure using our three musical perception tests, we computed structural equation models (SEMs) using the factor scores on the three perceptual tests as indicators of a latent variable general musical ability (\textit{g}) (see \textit{SI Figure 1}).
We constrained the intercepts and the loadings of the three tests on \textit{g} to be equal across men and women, tested the difference in the latent means of \textit{g} for significance, and computed Cohen's \textit{d} effect sizes.
This model estimated that the difference in \textit{g} between men and women was `r lv_diff_0 %>% round(digits = 3)`, with a pooled variance of `r S_0 %>% round(digits = 3)`.
This corresponds to an effect size of \textit{d} = `r cohensd_lv_sex_diffs %>% round(digits = 3)`, which is comparable to the above estimates derived from the GAM.

### Controlling for participant covariates

To test whether this magnitude of difference in \textit{g} may have been confounded by other participant variables, the above SEM was extended to include other participant variables that may have plausibly been influencing their test performance - specifically participants' age, education, age at which they started music lessons (if any), musical training, and musical listening. This SEM approach additionally allows us to model the correlational structure among the covariates (see \textit{SI Figure 1}). When taking covariate effects into account, the effect size increased slightly: the difference in \textit{g} after covariate adjustment was `r lv_diff_1 %>% round(digits = 3)`, with a pooled variance of `r S_1 %>% round(digits = 3)`.
This corresponds to an effect size of $d =$ `r cohensd_lv_sex_diffs_covariates %>% round(digits = 3)`.
This may reflect the influence of slight group differences on the above covariates.

### Testing whether differences were robust across covariate subgroups

Lastly, we tested whether the very small sex differences observed above were robust across different combinations of participants' covariates (namely: age, education, start age of music lessons, and factors musical training and musical listening).
To do this, we used a tree model based on recursive partitioning.
The tree model splits covariates into subgroups for which the coefficient for sex in a linear model changes significantly.
Given our large sample size, we set a significance threshold of p \< .001 and required a minimum subgroup to have at least 1% of the total sample size.
Tree models were automatically pruned using the Bayesian Information Criterion.
We computed the range of the effect sizes (Cohen's \textit{d}) across subgroups for the tree model of each dependent variable.

```{r Model-based recursive partitioning, cache = TRUE}
# MB: temporarily setting eval = FALSE so I can quickly render doc 
c_dat_red <- na.omit(c_dat[,c("gender_n",vars, "age","education_n","mus_train","lessons_age_n","mus_listen")])

mpt_abil_tree <- lmtree(mpt_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001, prune="BIC",cores=2)
# plot(lmtree(mpt_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2,maxdepth=4,parm=2))
range_mpt_abil <- range(coef(mpt_abil_tree)[,2] / sd(c_dat$mpt_abil))

mdt_abil_tree <- lmtree(mdt_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC", cores=2 )
# plot(lmtree(mdt_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2,maxdepth=4,parm=2))
range_mdt_abil <- range(coef(mdt_abil_tree)[,2] / sd(c_dat$mdt_abil))

cabat_abil_tree <- lmtree(cabat_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2 )
# plot(lmtree(cabat_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2,maxdepth=4,parm=2))
range_cabat_abil <- range(coef(cabat_abil_tree)[,2] / sd(c_dat$cabat_abil))

mus_abil_tree <- lmtree(mus_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen , data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2 )
# plot(lmtree(mus_ability ~ as.factor(gender_n) |   age + education_n+  mus_train + lessons_age_n + mus_listen, data = c_dat_red, minsize = n/100, alpha = 0.001,prune="BIC",cores=2,maxdepth=4,parm=2))
range_mus_abil <- range(coef(mus_abil_tree)[,2] / sd(c_dat$mus_ability))

range <- rbind(range_mpt_abil, range_mdt_abil, range_cabat_abil, range_mus_abil)

out <- data.frame(DV=vars,cohens_d_lb=range[,1],cohens_d_ub=range[,2])
# out

```

Most of these tree models formed subgroups based on participants' amount of musical training and start age of music lessons.
The range of effect sizes across these tree model subgroups ranged from Cohen's \textit{d} = `r min(abs(out %>% dplyr::select(cohens_d_lb, cohens_d_ub))) %>% round(digits = 3)` to `r max(abs(out %>% dplyr::select(cohens_d_lb, cohens_d_ub))) %>% round(digits = 3)`, this higher end representing effect sizes that are conventionally considered small effects (i.e. \textit{d} \> 0.2) [@Cohen1988].
Across these tree models, most models estimated a sex difference of Cohen's \textit{d} \< 0.2: even when controlling for other participant variables, sex differences on tests of general musical ability are very small.

These tree models reveal that subgroups constructed from these other variables reveal slightly larger effect sizes than those estimated in the full distribution.
These can be explained by the covariate musical training.
Musical training is positively related to musical ability, and females participants reported having higher musical training on average.
Hence, when data are conditioned on musical training (i.e. males and females are made statistically equal in terms of training), the estimated sex difference effect size slightly increased, with a small male advantage in most subgroups.
This appears to be a version of Simpson's paradox, where there are (almost) no differences in the full sample, but slightly stronger differences emerge for (almost) all subgroups analysed.

```{r effect training and listening}
c_dat_train <- dat %>%
# mutate new column that tags top and bottom 50% of music training 
            mutate(mus_train_group = ifelse(mus_train > quantile(c_dat$mus_train, probs = 0.5, na.rm = TRUE), "high", 
                 ifelse(mus_train < quantile(c_dat$mus_train, probs = 0.5, na.rm = TRUE) , "low", NA))) %>%
  # drop participants that are in the middle 80%
  filter(!is.na(mus_train_group))
vars <- c("mpt_ability","mdt_ability","cabat_ability","mus_ability")

if(file.exists("../results/model_coeffs_train.RData")){
  load("../results/model_coeffs_train.RData")
  } else{
      c_dat_red <- na.omit(c_dat_train[,c("mus_train_group",vars)])
      model_coeffs <- map_dfr(vars, function(v) gamlss(as.formula(sprintf(" %s ~ mus_train_group", v)), sigma.fo= ~ mus_train_group,family=NO(sigma.link="identity"),data = c_dat_red, trace=F) %>% broom::tidy())
      model_coeffs <- cbind(DV=rep(vars,each=4),model_coeffs)
      save(model_coeffs, file="../results/model_coeffs_train.RData")
  }
model_coeffs

gg_mpt_train <- ggplot(c_dat_train, aes(x=mpt_ability, fill=mus_train_group)) + geom_density(alpha=.3) +
  labs(x = "Mistuning Perception") +
  scale_fill_manual(name="Music Training", labels=c("High", "Low"), values=c("#E69F00", "#009E73")) +
  theme_minimal()
gg_mdt_train <- ggplot(c_dat_train, aes(x=mdt_ability, fill=mus_train_group)) + geom_density(alpha=.3)+
  labs(x = "Melodic Discrimination") + 
  scale_fill_manual(name="Music Training", labels=c("High", "Low"), values=c("#E69F00", "#009E73")) +
  theme_minimal()
gg_cabat_train <- ggplot(c_dat_train, aes(x=cabat_ability, fill=mus_train_group)) + geom_density(alpha=.3) +
  labs(x = "Beat Alignment") + 
  scale_fill_manual(name="Music Training", labels=c("High", "Low"), values=c("#E69F00", "#009E73")) +
  theme_minimal()
gg_mus_train <- ggplot(c_dat_train, aes(x=mus_ability, fill=mus_train_group)) + geom_density(alpha=.3) +
  labs(x = "General Musical Ability") + 
  scale_fill_manual(name="Music Training", labels=c("High", "Low"), values=c("#E69F00", "#009E73")) +
  theme_minimal()

if(file.exists("../results/effectsizes_train.RData")){
  load("../results/effectsizes_train.RData")
  } else{
    cohens_ds <- map_dfr(vars, function(v) cohens_d(as.formula(sprintf(" %s ~ mus_train_group", v)), data = c_dat_train, pooled_sd = F,verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    overlap <- map_dfr(vars, function(v) p_overlap(as.formula(sprintf(" %s ~ mus_train_group", v)), data = c_dat_train, parametric=F, verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    prob_superior <- map_dfr(vars, function(v) p_superiority(as.formula(sprintf(" %s ~ mus_train_group", v)), data = c_dat_train, parametric=T, verbose=F) %>% broom::tidy())[c(1,5,9,13),3]
    eta_sq <- map_dfr(vars, function(v) etasq(lm(as.formula(sprintf(" %s ~ mus_train_group", v)), data = c_dat_train)) %>% broom::tidy())[,3]
    r2 <- map_dfr(vars, function(v) summary(lm(as.formula(sprintf(" %s ~ mus_train_group", v)), data = c_dat_train))$r.squared %>% broom::tidy())
    effectsizes_train <- data.frame(DV=vars,cohens_d=cohens_ds$mean, pct_overlap=overlap$mean, prob_superior = prob_superior$mean, eta_sq=eta_sq$mean, R2 = r2$x)
    
  c_dat_low <- c_dat_train[c_dat_train$mus_train_group=="low",c("mpt_ability","mdt_ability","cabat_ability")]
  c_dat_high <- c_dat_train[c_dat_train$mus_train_group=="high",c("mpt_ability","mdt_ability","cabat_ability")]
  mv_es <- maha(c_dat_low, c_dat_high)
  mv_esq <- etasq(lm(cbind(mpt_ability,mdt_ability,cabat_ability) ~ mus_train_group, data=c_dat_train))
  effectsizes_train <- rbind(effectsizes_train,c("multivariate",mv_es$D,mv_es$OVL, mv_es$CL,mv_esq["mus_train_group",1], NA))
  save(effectsizes_train, file="../results/effectsizes_train.RData")
  }

effect_d_train <- effectsizes_train %>%
  as.data.frame() %>%
  dplyr::select(DV, cohens_d) %>%
  filter(DV != "multivariate") %>%
  mutate(cohens_d = as.numeric(cohens_d) %>% round(digits = 3))
```

```{r fig4, eval = FALSE, results = FALSE}
# fig.show="hold", out.width="100%", fig.cap="\\textbf{Fig. 4 | Distributions of general musical ability scores, by listening time} \\textit{`On an average day, how much time do you spend listening to music and/or watching videos that include music?'.}"

c_dat %>%
  mutate(listen_time = as.factor(music_listen_time_n)) %>%
  filter(!is.na(listen_time)) %>%
  ggplot(aes(x=mus_ability, fill = listen_time)) +
  geom_density(alpha=.3) +
  labs(x = "General Musical Ability") +
  scale_fill_manual(name="Listening Time", labels=c("No time at all", "1-5 minutes", "6-10 minutes", "11-15 minutes", "16-30 minutes", "31-60 minutes", "1-2 hours", "2-4 hours", "More than 4 hours"), values = c("red", "orange", "yellow", "green", "blue", "purple", "grey", "black")) +
  theme_minimal()
```

# End notes

## Data, code, and materials availability

A fully reproducible manuscript; data, analysis code, and visualizations; other materials; and code for the naïve listener experiment are available at <https://github.com/themusiclab/sex-differences>.
Readers may participate in the naïve listener experiment by visiting <https://themusiclab.org/quizzes/miq>.

## Acknowledgments
This research was supported by the Harvard Data Science Initiative (S.A.M.), the National Institutes of Health Director’s Early Independence Award DP5OD024566 (S.A.M), Natural Sciences and Engineering Research Council of Canada, Grant/Award Number: 05016 (J.T.S.), Fonds de Recherche du Québec Nature et Technologies, Grant/Award Number: PR-299652 (S.C.W.), an Anneliese Maier research prize awarded by the Humboldt Foundation (D.M.), and the Center for Research on Brain, Language, and Music (M.B.). We thank the participants, members of The Music Lab and The Peretz Lab for feedback, C. Hilton for technical assistance, and P. Harrison for raw data-preprocessing. 

## Author contributions
Conception S.A.M., D.M.; experimental design and implementation, S.A.M.; participant recruitment, data management, and data processing, S.A.M., D.M., M.B.; analysis and visualization, M.B., D.M.; writing, M.B., I.P., S.C.W., J.T.S., S.A.M.

## Supplemental Information

```{r SItable1 participant education, results = "asis"}
dat %>%
  dplyr::select(age, gender_n, education_n) %>%
  group_by(gender_n) %>% 
  summarise(N = n(),
            age_mean = round(mean(age), digits = 1),
            age_sd = round(sd(age), digits = 1),
            high_school = sum(education_n == 4, na.rm = TRUE), 
            some_undergrad = sum(education_n == 5, na.rm = TRUE),
            undergrad = sum(education_n %in% c(6,7), na.rm = TRUE), # complete high school + some graduate school
            postgrad = sum(education_n == 8, na.rm = TRUE)) %>%
  janitor::clean_names() %>%
  kable(.,
        format.args = list(big.mark = ","), # 1000 comma separator
        format = "latex",
        booktabs = TRUE,
        col.names = c("Gender", "n", "Mean Age (years)", "Age SD", "High Schoool", "Some Undergrad", "Undergrad", "Postgrad"),
        escape = FALSE, 
        linesep = "\\addlinespace",
        longtable = FALSE, # set to FALSE to stop this table being split across pages
        align=rep('l', 10))  %>%
        footnote(general = "Participant information, broken down by self-reported gender, age, and education.",
           general_title = "SI Table 1 |",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  add_header_above(header = c(" " = 4, "Highest education completed" = 4)) %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r SItable2 effect sizes with covariates, results = "asis"}
p_esq <-  map_dfr(vars, function(v) eta_squared(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))[1,"Eta2_partial"] %>% broom::tidy())
r2 <- map_dfr(vars, function(v) summary(lm(as.formula(sprintf(" %s ~ gender_n + mus_train + mus_listen  + age + education_n+ lessons_age_n", v)), data = c_dat))$r.squared %>% broom::tidy())


out <- data.frame(DV=vars, p_eta_sq_gender=p_esq$x, R2_full_model=r2$x)

out %>%
  mutate(DV = ifelse(DV == "mpt_ability", "Mistuning Perception", 
                     ifelse(DV == "mdt_ability", "Melodic Discrimination", 
                            ifelse(DV == "cabat_ability", "Beat Alignment", "General Music Ability")))) %>%
  mutate(p_eta_sq_gender = round(p_eta_sq_gender, digits = 3), 
         R2_full_model = round(R2_full_model, digits = 3)) %>%
  kable(.,
        format = "latex",
        booktabs = TRUE,
        escape = F,
        col.names = c("Task", "$\\eta^2$ of gender", "$R^2$ full model")) %>% 
        footnote(general = "The estimated gender effect remains of a comparable magnitude when other participant covariates - age, education, age at start of any music lessons, musical training, and musical listening - are added.",
           general_title = "SI Table 2 | Full Model, gender and covariates",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  kable_styling(position = "center", 
                latex_options = "HOLD_position")
```


```{r SEM, results = 'asis', out.width="100%", fig.show="hold", fig.cap="\\textbf{SI Figure 1 | Structural Equation Models} Top row shows gender-only models, bottom row shows all-covariate models. Left panels in each row show the models for female participants; right panels show these models for males."}
# gender only models
layout(t(1:2))
semPaths(lv_sex_diffs,intercepts=T, "eq", ask=F, intAtSide = T,"std",mar = c(8, 1, 5, 1))

# full models (gender + covariates)
layout(t(1:2))
semPaths(lv_sex_diffs_covariates,intercepts=T, "eq", ask=F, intAtSide = T, "est",mar = c(8, 1, 5, 1))
```

```{r SIfig2_per_sex, out.width= "100%", fig.cap = "\\textbf{SI Figure 2 | Distribution of General Musical Ability scores by self-assessed musical skill, divided by sex.}"}
c_dat %>%
  mutate(music_skill = as.factor(music_skill_n)) %>%
  filter(!is.na(music_skill)) %>%
  ggplot(aes(x=mus_ability, fill = music_skill)) +
  geom_density(alpha=.3) +
  labs(x = "General Musical Ability") +
  scale_fill_manual(name="Self-assessed Music Skill", labels=c("I have no skill at all", "I\'m a novice", "I have some skill", "I have a lot of skill", "I\'m an expert"), values = c("#E69F00", "#F0E442", "#009E73", "#56B4E9", "#CC79A7")) +
  theme_classic() + 
  facet_wrap(~gender_n) + 
  theme(legend.justification=c(0,1), legend.position=c(0,1), legend.background = element_rect(fill = "transparent", colour = "transparent"))
```

\newpage
```{r SItable3 prep}
# SI table detailing all items, their possible responses, and how the verbal responses were coded numerically
# MB: deliberately loading in plyr here, late, to not interfere with dplyr verbs higher up
library(plyr)
SI_dat <- readRDS("../data/clean-person-level-data-filtered.rds") %>% 
  # mutate to character to allow pivot longer
  mutate(music_listen = as.character(music_listen)) %>% 
  pivot_longer(everything(), names_to = "item", values_to = "answer") %>%
  filter(!is.na(answer)) %>%
  unique()

SI_dat_table <- SI_dat %>%
  mutate(verbal = NA,
         num = NA) %>%
  mutate(verbal = case_when(
    item == "music_tap" ~ "Can you tap in time with a musical beat?",
    item == "out_of_tune" ~ "When you are singing, can you tell if you are out-of-tune or off-key?",
    item == "music_skill" ~ "Think of your skill at making music (using a musical instrument or singing). How would you rate your own skill?", 
    item == "music_listen" ~ "How good do you think your music listening skills are? (things like remembering melodies, hearing out of tune notes, or hearing a beat that is out of sync with the music)",
    item == "lessons_enjoy" ~ "How much did you enjoy your music lessons?",
    item == "lessons_peers" ~ "Think about your peers in the time when you first did music lessons. How did your musical skills compare to theirs?",
    item == "theory_training" ~ "Have you ever had any music theory training?",
    item == "music_feel" ~ "Have you ever experienced \"chills\" or \"goosebumps\" in response to music?",
    item == "music_listen_time" ~ "On an average day, how much time do you spend listening to music and/or watching videos that include music?",
    item == "familiarity_tradmusic" ~ "How familiar are you with traditional music from around the world?"
  ))

SI_dat_table <- SI_dat_table %>%
  mutate(num = case_when(
    answer == "Yes" ~ 1,
    answer == "No" ~ 0,
    answer == "I'm not sure" ~ NA,
    # lessons_enjoy
    answer == "I disliked them a lot" ~ 1,
    answer == "I disliked them a little" ~ 2,
    answer == "I liked them a little" ~ 3,
    answer == "I liked them a lot" ~ 4,
    # lessons_peers
    answer == "They were a lot better than me" ~ 1,
    answer == "They were a little better than me" ~ 2,
    answer == "I was a little better than them" ~ 3,
    answer == "I was a lot better than them" ~ 4,
    # music_listen_time 
    answer == "No time at all" ~ 1,
    answer == "1-5 minutes" ~ 2,
    answer == "6-10 minutes" ~ 3,
    answer == "11-15 minutes" ~ 4,
    answer == "16-30 minutes" ~ 5,
    answer == "31-60 minutes" ~ 6,
    answer == "1-2 hours" ~ 7,
    answer == "2-4 hours" ~ 8,
    answer == "More than 4 hours" ~ 9,
    # music_skill
    answer == "I have no skill at all" ~ 1,
    answer == "I'm a novice" ~ 2,
    answer == "I have some skill" ~ 3,
    answer == "I have a lot of skill" ~ 4,
    answer == "I'm an expert" ~ 5,
    # theory_training
    answer == "No music theory training" ~ 1,
    answer == "A little music theory training" ~ 2,
    answer == "Some music theory training" ~ 3,
    answer == "A moderate amount of music theory training" ~ 4,
    answer == "A lot of music theory training" ~ 5,
    # familiarity_tradmusic
    answer == "I've never heard traditional music" ~ 1,
    answer == "I'm a little familiar with traditional music" ~ 2,
    answer == "I'm somewhat familiar with traditional music" ~ 3,
    answer == "I'm very familiar with traditional music" ~ 4,
    # music_feel (chills)
    answer == "No, never" ~ 1,
    answer == "Yes, but rarely" ~ 2,
    answer == "Yes, sometimes" ~ 3,
    answer == "Yes, often" ~ 4,
  ))

# music_listen was answered on a sliding scale, and then sliced into 6 segments in preprocessing
music_listen_add <- data.frame(item = rep("music_listen", 6),
                               answer = rep("'I'm much worse than other people' ---[sliding scale]--- 'I'm much better than other people'", 6),
                               verbal = rep("How good do you think your music listening skills are? (things like remembering melodies, hearing out of tune notes, or hearing a beat that is out of sync with the music)", 6),
                               num = c(1:6))
SI_dat_table <- SI_dat_table %>%
  filter(item != "music_listen") %>%
  full_join(music_listen_add)

# human readable item names 
SI_dat_table <- SI_dat_table %>%
  mutate(item = case_when(
    item == "music_tap" ~ "Tap In Time",
    item == "out_of_tune" ~ "Out Of Tune",
    item == "music_skill" ~ "Musical Skill",
    item == "music_listen" ~ "Music Listening Skill",
    item == "lessons_enjoy" ~ "Enjoyed Lessons", 
    item == "lessons_peers" ~ "Compared to Peers",
    item == "theory_training" ~ "Theory Training",
    item == "music_feel" ~ "Experience Chills",
    item == "music_listen_time" ~ "Listening Time",
    item == "familiarity_tradmusic" ~ "Familiarity Traditional Music"
  )) 

# putting in column to mark the conceptual groups
SI_dat_table <- SI_dat_table %>%
  mutate(factor = case_when(
    item %in% c("Tap In Time", "Out Of Tune", "Musical Skill", "Music Listening Skill") ~ "Self-reported Musical Ability",
    item %in% c("Enjoyed Lessons", "Compared to Peers", "Theory Training") ~ "Formal Musical Training",
    item %in% c("Experience Chills", "Listening Time", "Familiarity Traditional Music") ~ "Extent, Intensity, and Breadth of Musical Listening"))

SI_dat_table <- SI_dat_table %>%
# and reorder
  dplyr::select(factor, verbal, answer, num) %>% 
  dplyr::arrange(num) %>%
  dplyr::arrange(verbal) %>%
  dplyr::arrange(factor, by_group = TRUE) 
```

```{r SItable3, results = "asis"}
SI_dat_table %>%
  kable(.,
        format = "latex",
        booktabs = TRUE,
        col.names = c("Factor", "Item", "Response Options", "Numeric Coding"),
        escape = FALSE, 
        linesep = "\\addlinespace",
        longtable = TRUE,
        align=rep('l', 10)) %>%
        footnote(general = "Details of item verbal prompts, response options, and numeric coding of responses.",
           general_title = "SI Table 3 | Question Items",
           footnote_as_chunk = TRUE,
           title_format = "bold",
           escape = FALSE,
           threeparttable = TRUE) %>%
  column_spec(column = 1, width = "1.2in") %>%
  column_spec(column = 2, width = "1.9in") %>%
  column_spec(column = 3, width = "1.5in") %>%
  column_spec(column = 4, width = "0.5in") %>%
  collapse_rows() #%>%
#  kable_styling(full_width = TRUE)

```

```{r SI_fig_3_lessons, results = "asis", out.width="100%", fig.cap= "\\textbf{SI Figure 3 | Participants who had taken music lessons had higher musical ability than paticipants who had not.} There was a difference in general musical ability scores of d=0.684 between the those who self report having had or not had music lessons in their life."}
c_dat %>%
  mutate(music_lessons = as.factor(music_lessons_n)) %>%
  filter(!is.na(music_lessons)) %>%
  ggplot(aes(x=mus_ability, fill = music_lessons)) +
  geom_density(alpha=.3) +
  labs(x = "General Musical Ability") +
  scale_fill_manual(name="Music Lessons", labels=c("No", "Yes"), values = c("red", "green")) +
  theme_classic() + 
  labs(y = "Density") +
  theme(legend.justification=c(0,1), legend.position=c(0,1))
```
\newpage

# References
